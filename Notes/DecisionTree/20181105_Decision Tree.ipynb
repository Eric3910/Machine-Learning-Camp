{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "## 决策树的构造：\n",
    "### Feature: \n",
    "#### Pros:\n",
    "    -计算复杂度不高\n",
    "    -输出结果易于理解\n",
    "    -对中间值的缺失不敏感\n",
    "    -可以处理不相关特征数据\n",
    "#### Cons：\n",
    "    -可能会产生过度匹配问题\n",
    "#### Suiteable Data Type:\n",
    "    -数值型和标称型\n",
    "    \n",
    "#### 创建分支的伪代码函数createBranch()如下：\n",
    "    If so return ClassTag\n",
    "    Else\n",
    "        find the best feature to devide the dataset.\n",
    "        divide the dataset\n",
    "        create branch\n",
    "            for every divided sub-set\n",
    "                use createBranch and add the result returned to branch\n",
    "        return branch\n",
    "\n",
    "### 决策树的一般流程：\n",
    "    1. Data collecting: Any method\n",
    "    2. Data preparation: tree generation algorithm only suits for nominal data, so the numeric data should be discretized.\n",
    "    3. Data analysis: Any method. After tree generation, we should check the figure whether it fits for our prediction.\n",
    "    4. train algorithm: tree generation data structure.\n",
    "    5. test algorithm: use experience tree to calculate error rate.\n",
    "    6. use algorithm: this process is suitable for all supervised learning algorithm. Using decision tree could help better understand the inner meaning of the data.\n",
    "   \n",
    "### 信息增益 Information Gain\n",
    "\n",
    "- Information: $l(x_i)=-log_2p(x_i)$\n",
    "\n",
    "- Entropy（熵）: $H = -\\sum^n_{i=1}p(x_i)log_2p(x_i)$\n",
    "\n",
    "### 递归构建决策树\n",
    "1. 导入原始数据集\n",
    "2. 基于最好的属性值划分数据集\n",
    "3. 数据向下传递到树分支的下一个节点，再次划分\n",
    "##### 递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子节点或者终止块。\n",
    "##### 任何到达叶子节点的数据必然属于叶子节点的分类。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'N', 1: {'flippers': {0: 'N', 1: 'Y'}}}}\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    '''\n",
    "    Func : Calculate the Shannon Entropy for a given dataSet \n",
    "    '''\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    #create dict for every possible category\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] +=1\n",
    "    shannonEnt = 0.0\n",
    "    #probability, and log(prob), sum \n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        shannonEnt -= prob*log(prob,2)\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet():\n",
    "    dataSet = [[1,1,'Y'],\n",
    "            [1,1,'Y'],\n",
    "            [1,0,'N'],\n",
    "            [0,1,'N'],\n",
    "            [0,1,'N']]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    return dataSet, labels\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    '''\n",
    "    Func: divide the dataset based on given feature\n",
    "    '''\n",
    "    # create new list object \n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        # Extraction\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis] #extract the data from dataset where the given feature has given value\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    '''\n",
    "    choose the best feature to split\n",
    "    '''\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0 ; bestFeature = -1\n",
    "    #create unique feature list\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)# change list to set, keep unique keys.\n",
    "        newEntropy = 0.0\n",
    "        #calculate Entropy for every split way.\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob*calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #get bestInfoGain\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "def majorityCnt(classList):\n",
    "    classCount = {}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys(): classCount[vote]=0\n",
    "        classCount[vote] += 11\n",
    "    sortedClassCount = sorted(classCount.items(),\\\n",
    "     key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    #类别完全相同则停止继续划分\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    #遍历玩所有特征时返回出现次数最多的类别\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    #得到列表包含的所有属性值\n",
    "    del(labels[bestFeat])\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    uniqueVals = set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet\\\n",
    "                        (dataSet,bestFeat,value),subLabels)\n",
    "    return myTree\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    dataSet, labels = createDataSet()\n",
    "    myTree = createTree(dataSet,labels)\n",
    "    print(myTree)\n",
    "    \n",
    "if __name__ ==\"__main__\":\n",
    "\tmain()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### list.extend and list.append\n",
    "    -if there is two lists a and b\n",
    "    -a.append(b) means add the whole list b as one element in list a\n",
    "    -a.extend(b) means add all the elements in list b in list a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
