{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM 支持向量机（Support Vecotr Machines)\n",
    "### SMO算法(Sequential Minimal Optimization, 序列最小优化）\n",
    "- 优点: 泛化错误率第，计算开销不大，结果已解释。\n",
    "- 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅仅适用于处理二类问题\n",
    "- 适用数据类型: 数值型和标称型数据\n",
    "### 6.1 基于最大间隔分割数据\n",
    "\n",
    "- 将数据集分割开的直线称为分割超平面，数据集的维度N-1维，称为超平面，也就是分类的决策边界。\n",
    "- 如果数据点离决策平面越远，则最后的预测结果就越可信。\n",
    "- 找到离分割超平面最近的点，确保它们离分割面的距离尽可能元。该距离叫做间隔。\n",
    "- 支持向量就是离分隔超平面最近的哪些点。接下来就是试着最大化支持向量到分割面的距离。\n",
    "\n",
    "### 6.2 寻找最大间隔\n",
    "- 分隔超平面的形式可以写成 $\\textbf{w^Tx}+b$\n",
    "- 要计算点A到分割超平面的距离，就必须给出点到分割面的法线或垂线的长度，为$|\\textbf{w^TA}+b|/||\\textbf{w}||$\n",
    "- w和b描述了所给数据的分割线或超平面。\n",
    "#### 6.2.1 分类器求解的优化问题\n",
    "- 分类器输入数据后会输出一个类别标签，相当于一个类似于Sigmoid的函数在起作用。使用类似单位阶跃函数的函数作用于$\\textbf{w^Tx}+b$\n",
    "- u<0时f(u)输出-1，反之输出+1。只差符号，方便数学处理。\n",
    "- 间隔=label*$\\textbf{w^Tx}+b$,这样无论数据点处于正方向还是负方向，该值始终为一个较大的正数\n",
    "- 找出分类器定义的w和b。\n",
    "- 目的：先找到最小间隔的点，再让该点的间隔最大化，找到让间隔最大化的w和b\n",
    "- 该问题其实是在给定了一些约束条件后求最优值，对于这一类问题，通常使用拉格朗日乘子法\n",
    "\n",
    "#### 6.2.2 SVM应用的一般框架\n",
    "##### 一般流程：\n",
    "1. 收集数据：任意\n",
    "2. 准备数据：数值型数据\n",
    "3. 分析数据：有助于可视化分割超平面\n",
    "4. 训练算法： SVM的大部分时间用于训练，主要实现两个参数的调优\n",
    "5. 测试算法：很容易实现\n",
    "6. 使用算法：几乎适用于所有分类问题。SVM本身是二类分类器，对多类问题应用SVM需要修改代码\n",
    "\n",
    "### 6.3 SMO高效优化算法\n",
    "\n",
    "#### 6.3.1 Platt 的SMO算法\n",
    "* 大优化问题分解为多个小优化问题。\n",
    "* 目标是求出一系列的alpha和b\n",
    "* 工作原理： 每次循环选择两个alpha进行优化处理。一旦找到一对alpha，那么就增大其中一个同时减小另一个。\n",
    "\n",
    "#### 6.3.2 应用简化版SMO算法处理小规模数据集\n",
    "\n",
    "- 伪代码如下：\n",
    "\n",
    "- 创建一个alpha向量并将其初始化为0向量\n",
    "\n",
    "当迭代次数小于最大迭代次数时（外循环）\n",
    "\n",
    "   对数据集中的每个数据向量（内循环）：\n",
    "   \n",
    "    如果该数据向量可以被优化：\n",
    "     \n",
    "         随机选择一个另外一个数据向量\n",
    "         \n",
    "         同时优化这两个向量\n",
    "         \n",
    "         如果两个向量都不能被优化，退出内循环。\n",
    "         \n",
    "    如果所有向量都没被优化，增加迭代数目，继续下一次循环。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from time import sleep\n",
    "\n",
    "def loadDataSet(fileName):\n",
    "\tdataMat = []; labelMat = []\n",
    "\tfr = open(fileName)\n",
    "\tfor line in fr.readlines():\n",
    "\t\tlineArr = line.strip().split('\\t')\n",
    "\t\tdataMat.append(float(lineArr[0]),float(lineArr[1]))\n",
    "\t\tlabelMat.append(float(lineArr[2]))\n",
    "\treturn dataMat, labelMat\n",
    "\n",
    "def selectJrand(i,m):\n",
    "\tj = i\n",
    "\twhile(j == i):\n",
    "\t\tj = int(np.random.uniform(0,m))\n",
    "\treturn j\n",
    "\n",
    "def clipAlpha(aj,H,L):\n",
    "\tif aj > H:\n",
    "\t\taj = H\n",
    "\tif L > aj:\n",
    "\t\taj = L\n",
    "\treturn aj\n",
    "\n",
    "def smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n",
    "    '''\n",
    "    Func:\n",
    "    Paramaters:\n",
    "    dataMatIn: input data matrix\n",
    "    classLabels : class labels\n",
    "    C: a fixed number\n",
    "    toler: tolerance rate\n",
    "    maxIter: max iternate times\n",
    "    '''\n",
    "    # 标签转置为列向量。类别标签向量元素的每一行对应数据矩阵中的每一行。\n",
    "    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose() \n",
    "    #m,n是数据矩阵的大小。\n",
    "    b = 0; m,n = shape(dataMatrix)\n",
    "    #构建alpha矩阵，初始化元素为0.\n",
    "    alphas = mat(zeros((m,1)))\n",
    "    #iter变量用于记录alpha没有任何改变的情况下遍历数据集的次数\n",
    "    iter = 0\n",
    "    while (iter < maxIter):\n",
    "        alphaPairsChanged = 0\n",
    "        #对整个集合顺序遍历，变量alphaPairsChanged用于记录alpha是否已经进行优化。\n",
    "        for i in range(m):\n",
    "            #fXi是预测的类别，np.multiply是矩阵乘法，.T是矩阵转置。\n",
    "            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\n",
    "            # Ei是预估值与实际结果的误差。\n",
    "            Ei = fXi - float(labelMat[i])#if checks if an example violates KKT conditions\n",
    "            #随机选择第二个alpha = j，如果相对误差率超过容错率绝对值并且保证alphas[i]在0到C之间。\n",
    "            #因为如果达到0或者C，则说明已经在边界上，就不能再调整了。\n",
    "            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n",
    "                #selectJrand是用于随机选择一个j\n",
    "                j = selectJrand(i,m)\n",
    "                #再次计算在j下的分类结果\n",
    "                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\n",
    "                #计算j情况下实际值与预估值的误差\n",
    "                Ej = fXj - float(labelMat[j])\n",
    "                #旧的alpha和新的alpha通过copy()生成副本，如果简单赋值，只是传递了列表引用，后面的计算会更改，无法达到比较新旧矩阵的目的\n",
    "                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\n",
    "                #计算L和H，如果两个alpha对应的数据标签是相反的（标签只有两个结果）：\n",
    "                #否则如果数据标签是相同的，则L和H\n",
    "                if (labelMat[i] != labelMat[j]):\n",
    "                    L = max(0, alphas[j] - alphas[i])\n",
    "                    H = min(C, C + alphas[j] - alphas[i])\n",
    "                else:\n",
    "                    L = max(0, alphas[j] + alphas[i] - C)\n",
    "                    H = min(C, alphas[j] + alphas[i])\n",
    "                #如果L=H，就直接跳出本次，进入下一次的for循环\n",
    "                if L==H: print(\"L==H\"); continue\n",
    "                #eta是alpha[j]的最优修改量\n",
    "                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                if eta >= 0: print(\"eta>=0\"); continue\n",
    "                #基于eta对alphas[j]进行修改\n",
    "                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n",
    "                #利用clipAlpha对alphas[j]进行调整\n",
    "                alphas[j] = clipAlpha(alphas[j],H,L)\n",
    "                #判断alphas[j]跟调整前是否有轻微改变，如果是就跳出循环。\n",
    "                if (abs(alphas[j] - alphaJold) < 0.00001): print(\"j not moving enough\"); continue\n",
    "                #alphas[i]同样调整这么大，但是是反方向。alpha[j]减小，alpha[i]就增大\n",
    "                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])#update i by the same amount as j\n",
    "                                                                        #the update is in the oppostie direction\n",
    "                #为这两个alpha设置常数项b\n",
    "                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n",
    "                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n",
    "                if (0 < alphas[i]) and (C > alphas[i]): b = b1\n",
    "                elif (0 < alphas[j]) and (C > alphas[j]): b = b2\n",
    "                else: b = (b1 + b2)/2.0\n",
    "                #如果程序执行到这里还没有跳出，说明该alpha对已经成功改变。\n",
    "                alphaPairsChanged += 1\n",
    "                print(\"iter: %d i:%d, pairs changed %d\" % (iter,i,alphaPairsChanged))\n",
    "        #如果本次循环没有改变alpha对的值，就增加迭代次数再次for循环\n",
    "        if (alphaPairsChanged == 0): iter += 1\n",
    "        #如果改变了alpha对的值，就iter置零，继续运行。\n",
    "        else: iter = 0\n",
    "        print(\"iteration number: %d\" % iter)\n",
    "    #最终，只有在整个数据集上遍历maxIter次，而且不再发生任何alpha修改以后，程序才会退出while循环\n",
    "    return b,alphas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
