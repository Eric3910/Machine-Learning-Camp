{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "### 利用Logistic 回归进行分类的主要思想：\n",
    "根据现有数据对分类边界线建立回归公式，以此进行分类。\n",
    "\n",
    "训练分类器的做法就是寻找最佳你和参数，使用的是最优化算法。\n",
    "\n",
    "### Logistic回归的一般过程：\n",
    "1. 收集数据：任意\n",
    "2. 准备数据：由于要进行距离计算，所以需要数据为数值型。结构化数据最佳\n",
    "3. 分析数据：任意\n",
    "4. 训练算法： 大部分时间用于训练，训练的目的是为了找到最佳的分类回归系数\n",
    "5. 测试算法：训练完成后，分类很快\n",
    "6. 使用算法： 首先，需要input data,将其转化为对应的结构化数值。\n",
    "接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定他们属于哪个类别。在这之后，就可以在输出类别上做其他工作。\n",
    "\n",
    "\n",
    "### 5.1 基于logistic回归和Sigmoid函数的分类\n",
    "### Logistic 回归：\n",
    "- 优点： 计算代价不高，易于理解实现\n",
    "- 缺点：容易欠拟合，分类精度可能不高\n",
    "- 适用数据类型： 数值型和标称型数据\n",
    "\n",
    "\n",
    "### Sigmoid 函数\n",
    "#### $ \\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "#### 性质：x=0时y=0.5，随着x增大，y趋近于1，随着x减小，y趋近于0.如果横坐标足够大，则类似于阶跃函数\n",
    "\n",
    "- 为了实现Logistic回归分类器，可以在每个特征上乘以一个回归系数，然后将所有的结果值相加。总和带入Sigmoid函数。得到一个范围在0~1之间的数值。大于0.5的为分类1，小于0.5则为分类0。所以，Logistic回归也看做是概率估计。\n",
    "### 最佳回归系数\n",
    "\n",
    "$ z=w_0x_0 + w_1x_1+...+w_nx_n$\n",
    "\n",
    "$ z=w^Tx$ (向量写法）\n",
    "\n",
    "- 相当于两个数值向量对应元素相乘后$\\Sigma$.其中向量x是分类器的输入数据，向量w是要找的最佳参数\n",
    "\n",
    "#### 5.2.1 梯度上升法;\n",
    "##### 思想： 要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度为$\\Delta$,则函数f(x,y)的梯度为：\n",
    "- 沿着x的方向移动f(x,y)对x的偏导数，沿着y的方向移动f(x,y)对y的偏导，到下一点后继续，直到满足停止条件。\n",
    "\n",
    "梯度算子总是指向函数值增长最快的方向。移动量的大小叫做步长$\\alpha$.梯度上升算法的迭代公式为：\n",
    "$$w := w + \\alpha\\Delta_wf(w)$$\n",
    "\n",
    "对于梯度下降算法而言，公式中的加法变成减法，用于求函数的最小值。\n",
    "$$w := w - \\alpha\\Delta_wf(w)$$\n",
    "\n",
    "#### 5.2.2 训练算法：使用梯度上升找到最佳参数\n",
    "##### 伪代码如下：\n",
    "每个回归系数初始化为1\n",
    "\n",
    "重复R次：\n",
    "    \n",
    "    计算整个数据集的梯度\n",
    "    \n",
    "    使用alpha * Gradient更新回归系数的向量\n",
    "返回回归系数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 训练算法：随机梯度上升\n",
    "##### 伪代码：\n",
    "\n",
    "所有回归系数初始化为1\n",
    "\n",
    "对数据集中每个样本：\n",
    "    \n",
    "    计算该样本的梯度\n",
    "    使用alpha*gradient更新回归系数值\n",
    "  \n",
    "返回回归系数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 实例：从疝气病症预测病马的死亡率\n",
    "##### 5.3.1 准备数据：处理数据中的缺失值\n",
    "\n",
    "可选做法：\n",
    "- 使用可用特征的均值填补缺失值\n",
    "- 使用特殊值来填补缺失值，如-1\n",
    "- 忽略有缺失值的样本\n",
    "- 使用相似样本的均值添补缺失值\n",
    "- 使用其他算法预测缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error Rate of this test is : 0.283582\n",
      "The error Rate of this test is : 0.388060\n",
      "The error Rate of this test is : 0.462687\n",
      "The error Rate of this test is : 0.358209\n",
      "The error Rate of this test is : 0.343284\n",
      "The error Rate of this test is : 0.343284\n",
      "The error Rate of this test is : 0.388060\n",
      "The error Rate of this test is : 0.313433\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadDataSet():\n",
    "\tdataMat = []; labelMat = []\n",
    "\tfr = open('testSet.txt')\n",
    "\tfor line in fr.readlines():\n",
    "\t\tlineArr = line.strip().split()\n",
    "\t\tdataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n",
    "\t\tlabelMat.append(int(lineArr[2]))\n",
    "\treturn dataMat, labelMat\n",
    "\n",
    "def sigmoid(inX):\n",
    "\treturn 1.0/(1+np.exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "\tdataMatrix = np.mat(dataMatIn)\n",
    "\tlabelMat = np.mat(classLabels).transpose() # 对label行向量转置\n",
    "\tm,n = np.shape(dataMatrix)\n",
    "\talpha = 0.001 # 移动步长\n",
    "\tmaxCycles = 500 # 迭代次数\n",
    "\tweights = np.ones((n,1))\n",
    "\tfor k in range(maxCycles):\n",
    "\t\th = sigmoid(dataMatrix * weights)\n",
    "\t\terror = (labelMat - h)\n",
    "\t\tweights = weights + alpha * dataMatrix.transpose() * error\n",
    "\treturn weights\n",
    "\n",
    "def plotBestFit(weights):\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tdataMat, labelMat = loadDataSet()\n",
    "\tdataArr = np.array(dataMat)\n",
    "\tn = np.shape(dataArr)[0]\n",
    "\txcord1 = []; ycord1 = []\n",
    "\txcord2 = []; ycord2 = []\n",
    "\tfor i in range(n):\n",
    "\t\tif int(labelMat[i]) == 1:\n",
    "\t\t\txcord1.append(dataArr[i,1]); ycord1.append(dataArr[i,2])\n",
    "\t\telse:\n",
    "\t\t\txcord2.append(dataArr[i,1]); ycord2.append(dataArr[i,2])\n",
    "\tfig = plt.figure()\n",
    "\tax = fig.add_subplot(111)\n",
    "\tax.scatter(xcord1,ycord1, s=30 ,c ='red', marker='s')\n",
    "\tax.scatter(xcord2,ycord2, s=30 ,c ='green')\n",
    "\tx = arange(-3.0,3.0,0.1)\n",
    "\ty = (-weights[0]-weights[1]*x)/weights[2]\n",
    "\tax.plot(x,y)\n",
    "\tplt.xlabel('X1')\n",
    "\tplt.xlabel('X2')\n",
    "\tplt.show()\n",
    "\n",
    "def stocGradAscent0(dataMatrix, classLabels):\n",
    "\tm,n = np.shape(dataMatrix)\n",
    "\talpha = 0.01\n",
    "\tweights = ones(n)\n",
    "\tfor i in range(n):\n",
    "\t\th = sigmoid(sum(dataMatrix[i]*weights))\n",
    "\t\terror = classLabels[i] - h\n",
    "\t\tweights = weights + alpha * error * dataMatrix[i]\n",
    "\treturn weights\n",
    "\n",
    "def stocGradAscent1(dataMatrix, classLabels, numIter):\n",
    "\tm,n = np.shape(dataMatrix)\n",
    "\tweights = np.ones(n)\n",
    "\tfor j in range(numIter):\n",
    "\t\tdataIndex = list(range(m))\n",
    "\t\tfor i in range(m):\n",
    "\t\t\talpha = 4/(1.0+j+i)+0.01 # alpha 随着迭代次数逐渐减小，并且，j是迭代次数，i是样本点的下标。这样当j<<max(i)的时候，alpha不是严格下降。\n",
    "\t\t\trandIndex = int(np.random.uniform(0,len(dataIndex)))# 选择随机数\n",
    "\t\t\th = sigmoid(sum(dataMatrix[randIndex]*weights)) \n",
    "\t\t\terror = classLabels[randIndex] - h\n",
    "\t\t\tweights = weights + alpha * error * dataMatrix[randIndex]\n",
    "\t\t\tdel(dataIndex[randIndex])#将该样本删除\n",
    "\treturn weights\n",
    "\n",
    "def classifyVector(inX, weights):\n",
    "    prob = sigmoid(sum(inX*weights))\n",
    "    if prob > 0.5: return 1.0\n",
    "    else: return 0.0\n",
    "\n",
    "\n",
    "\n",
    "def colicTest():\n",
    "\tfrTrain = open('horseColicTraining.txt')\n",
    "\tfrTest = open('horseColicTest.txt')\n",
    "\ttrainingSet = [] ; trainingLabels = []\n",
    "\tfor line in frTrain.readlines():\n",
    "\t\tcurrLine = line.strip().split('\\t')\n",
    "\t\tlineArr = []\n",
    "\t\tfor i in range(21):\n",
    "\t\t\tlineArr.append(float(currLine[i]))\n",
    "\t\ttrainingSet.append(lineArr)\n",
    "\t\ttrainingLabels.append(float(currLine[21]))\n",
    "\ttrainWeights = stocGradAscent1(np.array(trainingSet), trainingLabels,500)\n",
    "\terrorCount = 0; numTestVec = 0.0\n",
    "\tfor line in frTest.readlines():\n",
    "\t\tnumTestVec += 1.0\n",
    "\t\tcurrLine = line.strip().split('\\t')\n",
    "\t\tlineArr = []\n",
    "\t\tfor i in range(21):\n",
    "\t\t\tlineArr.append(float(currLine[i]))\n",
    "\t\tif int(classifyVector(np.array(lineArr),trainWeights))!= int(currLine[21]):\n",
    "\t\t\terrorCount += 1\n",
    "\terrorRate = (float(errorCount)/numTestVec)\n",
    "\tprint(\"The error Rate of this test is : %f\"%errorRate)\n",
    "\treturn errorRate\n",
    "\n",
    "def multiTest():\n",
    "\tnumTests = 10; errorSum = 0.0\n",
    "\tfor k in range(numTests):\n",
    "\t\terrorSum += colicTest()\n",
    "\tprint(\"after %d iterations the average error rate is %f\"%(numTests,errorSum/float(numTests)))\n",
    "\n",
    "def main():\n",
    "    multiTest()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
