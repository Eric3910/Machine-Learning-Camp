{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树\n",
    "## 决策树的构造：\n",
    "### Feature: \n",
    "#### Pros:\n",
    "    -计算复杂度不高\n",
    "    -输出结果易于理解\n",
    "    -对中间值的缺失不敏感\n",
    "    -可以处理不相关特征数据\n",
    "#### Cons：\n",
    "    -可能会产生过度匹配问题\n",
    "#### Suiteable Data Type:\n",
    "    -数值型和标称型\n",
    "    \n",
    "#### 创建分支的伪代码函数createBranch()如下：\n",
    "    If so return ClassTag\n",
    "    Else\n",
    "        find the best feature to devide the dataset.\n",
    "        divide the dataset\n",
    "        create branch\n",
    "            for every divided sub-set\n",
    "                use createBranch and add the result returned to branch\n",
    "        return branch\n",
    "\n",
    "### 决策树的一般流程：\n",
    "    1. Data collecting: Any method\n",
    "    2. Data preparation: tree generation algorithm only suits for nominal data, so the numeric data should be discretized.\n",
    "    3. Data analysis: Any method. After tree generation, we should check the figure whether it fits for our prediction.\n",
    "    4. train algorithm: tree generation data structure.\n",
    "    5. test algorithm: use experience tree to calculate error rate.\n",
    "    6. use algorithm: this process is suitable for all supervised learning algorithm. Using decision tree could help better understand the inner meaning of the data.\n",
    "   \n",
    "### 信息增益 Information Gain\n",
    "\n",
    "- Information: $l(x_i)=-log_2p(x_i)$\n",
    "\n",
    "- Entropy（熵）: $H = -\\sum^n_{i=1}p(x_i)log_2p(x_i)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "\n",
    "def calcShannonEnt(dataSet):\n",
    "    '''\n",
    "    Func : Calculate the Shannon Entropy for a given dataSet \n",
    "    '''\n",
    "    numEntries = len(dataSet)\n",
    "    labelCounts = {}\n",
    "    #create dict for every possible category\n",
    "    for featVec in dataSet:\n",
    "        currentLabel = featVec[-1]\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel] = 0\n",
    "        labelCounts[currentLabel] +=1\n",
    "    shannonEnt = 0.0\n",
    "    #probability, and log(prob), sum \n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key])/numEntries\n",
    "        shannonEnt -= prob*log(prob,2)\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet():\n",
    "    dataSet = [[1,1,'Y'],\n",
    "            [1,1,'Y'],\n",
    "            [1,0,'N'],\n",
    "            [0,1,'N'],\n",
    "            [0,1,'N']]\n",
    "    labels = ['no surfacing','flippers']\n",
    "    return dataSet, labels\n",
    "\n",
    "def splitDataSet(dataSet, axis, value):\n",
    "    '''\n",
    "    Func: divide the dataset based on given feature\n",
    "    '''\n",
    "    # create new list object \n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        # Extraction\n",
    "        if featVec[axis] == value:\n",
    "            reducedFeatVec = featVec[:axis] #extract the data from dataset where the given feature has given value\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    '''\n",
    "    choose the best feature to split\n",
    "    '''\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    bestInfoGain = 0.0 ; bestFeature = -1\n",
    "    #create unique feature list\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)# change list to set, keep unique keys.\n",
    "        newEntropy = 0.0\n",
    "        #calculate Entropy for every split way.\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            prob = len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy += prob*calcShannonEnt(subDataSet)\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        #get bestInfoGain\n",
    "        if (infoGain > bestInfoGain):\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "def main():\n",
    "    dataSet, labels = createDataSet()\n",
    "    #print(dataSet)\n",
    "    #print(labels)\n",
    "    #print(calcShannonEnt(dataSet))\n",
    "    #print(splitDataSet(dataSet,0,1))\n",
    "    #print(splitDataSet(dataSet,0,0))\n",
    "    print(chooseBestFeatureToSplit(dataSet))\n",
    "if __name__ ==\"__main__\":\n",
    "\tmain()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list.extend and list.append\n",
    "    -if there is two lists a and b\n",
    "    -a.append(b) means add the whole list b as one element in list a\n",
    "    -a.extend(b) means add all the elements in list b in list a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
